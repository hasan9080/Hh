---
reviewers:
- caesarxuchao
- dchen1107
title: Вузли 
aka: Nodes
content_type: concept
weight: 10
---

<!-- overview -->

Kubernetes виконує ваше {{< glossary_tooltip text="навантаження" term_id="workload" >}} шляхом розміщення контейнерів у Podʼах для запуску на _Вузлах_. Вузол може бути віртуальною або фізичною машиною, залежно від кластера. Кожен вузол керується {{< glossary_tooltip text="панеллю управління" term_id="control-plane" >}} і містить необхідні служби для запуску {{< glossary_tooltip text="Podʼів" term_id="pod" >}}.

Зазвичай в кластері є кілька вузлів; в умовах навчання чи обмежених ресурсів може бути всього один вузол.

[Компоненти](/docs/concepts/overview/components/#node-components) на вузлі включають {{< glossary_tooltip text="kubelet" term_id="kubelet" >}}, {{< glossary_tooltip text="середовище виконання контейнерів" term_id="container-runtime" >}} та
{{< glossary_tooltip text="kube-proxy" term_id="kube-proxy" >}}.

<!-- body -->

## Управління {#managmenet}

Є два основних способи додавання Вузлів до {{< glossary_tooltip text="API-сервера" term_id="kube-apiserver" >}}:

1. kubelet на вузлі самостійно реєструється в панелі управління.
2. Ви (або інший користувач) вручну додаєте обʼєкт Node.

Після створення {{< glossary_tooltip text="обʼєкта" term_id="object" >}} Node, або якщо kubelet на вузлі самостійно реєструється, панель управління перевіряє, чи новий обʼєкт Node є дійсним. Наприклад, якщо ви спробуєте створити Вузол з наступним JSON-маніфестом:

```json
{
  "kind": "Node",
  "apiVersion": "v1",
  "metadata": {
    "name": "10.240.79.157",
    "labels": {
      "name": "my-first-k8s-node"
    }
  }
}
```

Kubernetes внутрішньо створює обʼєкт Node. Kubernetes перевіряє чи kubelet зареєструвався в API-сервері, що відповідає полю `metadata.name` Node. Якщо вузол є справним (тобто всі необхідні служби працюють), то він може запускати Podʼи. В іншому випадку цей вузол ігнорується для будь-якої діяльності кластера доки він не стане справним.

{{< note >}}
Kubernetes зберігає обʼєкт для недійсного Вузла та продовжує перевіряти, чи він стає справним.

Вам або {{< glossary_tooltip term_id="controller" text="контролер" >}} має явно видалити обʼєкт Node, щоб припинити цю перевірку його справності.
{{< /note >}}

Назва обʼєкта Node повинно бути дійсним [імʼям DNS-піддомену](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

### Унікальність назв Вузлів {#node-name-uniqueness}

[Назва](/docs/concepts/overview/working-with-objects/names#names) ідентифікує Node. Два Вузли не можуть мати однакову назву одночасно. Kubernetes також припускає, що ресурс з такою ж назвою — це той самий обʼєкт. У випадку Вузла припускається неявно, що екземпляр, який використовує ту ж назву, матиме той самий стан (наприклад, мережеві налаштування, вміст кореневого диска) та атрибути, такі як мітки вузла. Це може призвести до невідповідностей, якщо екземпляр був змінений без зміни його назви. Якщо Вузол потрібно замінити або значно оновити, наявний обʼєкт Node повинен бути видалений з API-сервера спочатку і знову доданий після оновлення.

### Самореєстрація Вузлів {#self-registration-of-nodes}

Коли прапорець kubelet `--register-node` є true (типово), kubelet спробує зареєструвати себе в API-сервері. Цей підхід використовується більшістю дистрибутивів.

Для самореєстрації kubelet запускається з наступними параметрами:

- `--kubeconfig` — Шлях до облікових даних для автентифікації на API-сервері.
- `--cloud-provider` — Як взаємодіяти з {{< glossary_tooltip text="хмарним постачальником" term_id="cloud-provider" >}} для отримання метаданих про себе.
- `--register-node` — Автоматична реєстрація в API-сервері.
- `--register-with-taints` — Реєстрація вузла з заданим списком {{< glossary_tooltip text="позначок" term_id="taint" >}} (розділених комами `<ключ>=<значення>:<ефект>`).

  Нічого не відбувається, якщо `register-node` є false.
- `--node-ip` — Необовʼязковий розділений комами список IP-адрес вузла. Можна вказати лише одну адресу для кожного роду адрес. Наприклад, у кластері з одним стеком IPv4 ви встановлюєте це значення як IPv4-адресу, яку повинен використовувати kubelet для вузла. Див. [налаштування подвійного стека IPv4/IPv6](/docs/concepts/services-networking/dual-stack/#configure-ipv4-ipv6-dual-stack) для отримання відомостей з запуску кластера з подвійним стеком.

  Якщо ви не вказали цей аргумент, kubelet використовує стандартну IPv4-адресу вузла, якщо є; якщо у вузла немає адреси IPv4, тоді kubelet використовує стандартну IPv6-адресу вузла.
- `--node-labels` - {{< glossary_tooltip text="Мітки" term_id="label" >}} для додавання при реєстрації вузла в кластері (див. обмеження міток, що накладаються [втулком доступу NodeRestriction](/docs/reference/access-authn-authz/admission-controllers/#noderestriction)).
- `--node-status-update-frequency` — Вказує, як часто kubelet публікує свій статус вузла на API-сервері.

Коли увімкнено [режим авторизації Вузла](/docs/reference/access-authn-authz/node/) та [втулок доступу NodeRestriction](/docs/reference/access-authn-authz/admission-controllers/#noderestriction), kubelets мають право створювати/змінювати лише свій власний ресурс Node.

{{< note >}}
Як зазначено в розділі [Унікальність назв Вузлів](#node-name-uniqueness), коли потрібно оновити конфігурацію Вузла, добре було б знову зареєструвати вузол в API-сервері. Наприклад, якщо kubelet перезапускається з новим набором `--node-labels`, але використовується та ж назва Node, зміна не відбудеться, оскільки мітки встановлюються при реєстрації вузла.

Podʼи, вже заплановані на Node, можуть погано поводитися або викликати проблеми, якщо конфігурація Node буде змінена при перезапуску kubelet. Наприклад, вже запущений Pod може бути позначений новими мітками, призначеними Node, тоді як інші Pod, які несумісні з цим Pod, будуть заплановані на основі цієї нової мітки. Перереєстрація вузла гарантує, що всі Podʼи будуть очищені та належним чином переплановані.
{{< /note >}}

### Ручне адміністрування Вузлів {#manual-node-administration}

Ви можете створювати та змінювати обʼєкти Node за допомогою {{< glossary_tooltip text="kubectl" term_id="kubectl" >}}.

Коли ви хочете створювати обʼєкти Node вручну, встановіть прапорець kubelet `--register-node=false`.

Ви можете змінювати обʼєкти Node незалежно від налаштувань `--register-node`. Наприклад, ви можете встановлювати мітки на наявному Вузлі або позначати його як незапланований.

Ви можете використовувати мітки на Вузлах разом із селекторами вузлів на Podʼах для управління плануванням. Наприклад, ви можете обмежити Pod лише можливістю запуску на
підмножині доступних вузлів.

Позначення вузла як незапланованого перешкоджає планувальнику розміщувати нові Podʼи на цьому Вузлі, але не впливає на наявні Podʼи на Вузлі. Це корисно як підготовчий крок перед перезавантаженням вузла чи іншим обслуговуванням.

Щоб позначити Вузол як незапланований, виконайте:

```shell
kubectl cordon $NODENAME
```

Див. [Безпечне очищення вузла](/docs/tasks/administer-cluster/safely-drain-node/) для деталей.

{{< note >}}
Podʼи, які є частиною {{< glossary_tooltip term_id="daemonset" >}}, можуть працювати на незапланованому Вузлі. Зазвичай DaemonSets надають служби, що працюють локально на Вузлі, навіть якщо він очищується від робочих навантажень.
{{< /note >}}

## Статус Вузла {#node-status}

Статус Вузла містить наступну інформацію:

- [Адреси](/docs/reference/node/node-status/#addresses)
- [Умови](/docs/reference/node/node-status/#condition)
- [Місткість та Розподіленість](/docs/reference/node/node-status/#capacity)
- [Інформація](/docs/reference/node/node-status/#info)

Ви можете використовувати `kubectl`, щоб переглядати статус Вузла та інші деталі:

```shell
kubectl describe node <вставте-назву-вузла-тут>
```

Див. [Статус Вузла](/docs/reference/node/node-status/) для отримання додаткової інформації.

## Сигнали Вузлів {#node-heartbeats}

Сигнали, надсилані вузлами Kubernetes, допомагають вашому кластеру визначати доступність кожного вузла та вживати заходів у випадку виявлення відмов.

Для вузлів існують дві форми сигналів:

- Оновлення в [`.status`](/docs/reference/node/node-status/) Вузла.
- Обʼєкти [Оренди (Lease)](/docs/concepts/architecture/leases/) у просторі імен `kube-node-lease`. Кожен Вузол має асоційований обʼєкт Lease.

## Контролер вузлів {#node-controller}

Контролер вузлів — це компонент панелі управління Kubernetes, який керує різними аспектами роботи вузлів.

У контролера вузла є кілька ролей у житті вузла. По-перше, він призначає блок CIDR вузлу при його реєстрації (якщо призначення CIDR увімкнено).

По-друге, він підтримує актуальність внутрішнього списку вузлів контролера зі списком доступних машин хмарного провайдера. У разі, якщо вузол несправний, контролер вузла перевіряє у хмарному провайдері, чи ще доступний віртуальний компʼютер (VM) для цього вузла. Якщо ні, контролер вузла видаляє вузол зі свого списку вузлів.

По-третє, контролер відповідає за моніторинг стану вузлів і:

- У випадку, якщо вузол стає недоступним, оновлення умови `Ready` у полі `.status` Вузла. У цьому випадку контролер вузла встановлює умову `Ready` в `Unknown`.
- Якщо вузол залишається недоступним: запускає [ініційоване API вивільнення](/docs/concepts/scheduling-eviction/api-eviction/) для всіх Podʼів на недосяжному вузлі. Типово контролер вузла чекає 5 хвилин між позначенням вузла як `Unknown` та поданням першого запиту на вивільнення.

Стандартно контролер вузла перевіряє стан кожного вузла кожні 5 секунд. Цей період можна налаштувати за допомогою прапорця `--node-monitor-period` у компоненті `kube-controller-manager`.

### Обмеження швидкості вивільнення {#rate-limits-on-eviction}

У більшості випадків контролер вузла обмежує швидкість вивільнення на `--node-eviction-rate` (типово 0,1) в секунду, що означає, що він не буде виводити Podʼи з більше, ніж 1 вузла кожні 10 секунд.

Поведінка вивільнення вузла змінюється, коли вузол в певній доступності стає несправним. Контролер вузла перевіряє, яка частина вузлів в зоні є несправною (умова `Ready` — `Unknown` або `False`) у той самий час:

- Якщо частка несправних вузлів становить принаймні `--unhealthy-zone-threshold` (типово 0,55), тоді швидкість вивільнення зменшується.
- Якщо кластер малий (тобто має менше або дорівнює `--large-cluster-size-threshold` вузлів — типово 50), тоді вивільнення припиняється.
- У іншому випадку швидкість вивільнення зменшується до `--secondary-node-eviction-rate` (типово 0,01) в секунду.

Причина того, що ці політики реалізовані окремо для кожної зони доступності, полягає в тому, що одна зона може втратити зʼєднання з панеллю управління, тоді як інші залишаються підключеними. Якщо ваш кластер не охоплює кілька зон доступності хмарного провайдера, тоді механізм вивільнення не враховує доступність поміж зон.

Однією з ключових причин розподілу вузлів за зонами доступності є можливість переміщення навантаження в справні зони, коли одна ціла зона виходить з ладу. Таким чином, якщо всі вузли в зоні несправні, контролер вузла виводить навантаження зі звичайною швидкістю `--node-eviction-rate`. Крайній випадок — коли всі зони повністю несправні (жоден вузол в кластері не є справним). У такому випадку контролер вузла припускає, що існує якась проблема зі зʼєднанням між панеллю управління та вузлами, і не виконує жодних вивільнень. (Якщо стався збій і деякі вузли відновились, контролер вузла виводить Podʼи з решти вузлів, які є несправними або недосяжними).

Контролер вузла також відповідає за виведення Podʼів, що працюють на вузлах із позначками `NoExecute`, якщо ці Podʼи дозволяють таке. Контролер вузла також додає {{< glossary_tooltip text="taint" term_id="taint" >}}, відповідні проблемам вузла, таким як недосяжність вузла або неготовність вузла. Це означає, що планувальник не розміщуватиме Podʼи на несправних вузлах.

## Відстеження місткості ресурсів {#node-capacity}

Обʼєкти Node відстежують інформацію про ресурсну місткість вузла: наприклад, кількість доступної памʼяті та кількість процесорів. Вузли, які [реєструються самостійно](#self-registration-of-nodes), повідомляють про свою місткість під час реєстрації. Якщо ви [додаєте вузол вручну](#manual-node-administration), то вам потрібно встановити інформацію про місткість вузла при його додаванні.

Планувальник Kubernetes {{< glossary_tooltip text="scheduler" term_id="kube-scheduler" >}} забезпечує, що на вузлі є достатньо ресурсів для всіх Podʼів. Планувальник перевіряє, що сума запитів контейнерів на вузлі не перевищує місткості вузла. Ця сума запитів включає всі контейнери, керовані kubelet, але не включає жодні контейнери, запущені безпосередньо середовищем виконання контейнерів, а також виключає будь-які процеси, які працюють поза контролем kubelet.

{{< note >}}
Якщо ви хочете явно зарезервувати ресурси для не-Pod процесів, дивіться
[резервування ресурсів для системних служб](/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved).
{{< /note >}}

## Топологія вузла

{{< feature-state feature_gate_name="TopologyManager" >}}

Якщо ви увімкнули [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) ресурсу `TopologyManager`, то kubelet може використовувати підказки топології при прийнятті рішень щодо призначення ресурсів. Див. [Керування політиками топології на вузлі](/docs/tasks/administer-cluster/topology-manager/)
для отримання додаткової інформації.

## Відповідне вимикання вузла {#graceful-node-shutdown}

{{< feature-state feature_gate_name="GracefulNodeShutdown" >}}

Kubelet намагається виявити вимикання системи вузла та завершує виконання Podʼів на вузлі.

Kubelet забезпечує виконання нормального [процесу завершення роботи Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) під час вимикання вузла. Під час вимикання вузла kubelet не приймає нові Podʼи (навіть якщо ці Podʼи вже призначені вузлу).

Можливість відповідного вимикання вузла залежить від systemd, оскільки вона використовує [блокування інгібіторів systemd](https://www.freedesktop.org/wiki/Software/systemd/inhibit/) для затримки вимкнення вузла на певний час.

Вимикання вузла керується властивістю
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `GracefulNodeShutdown`, яка є типово увімкненою з версії 1.21.

Зауважте, що типово обидва налаштування конфігурації, описані нижче, `shutdownGracePeriod` та `shutdownGracePeriodCriticalPods`, встановлені на нуль, таким чином, не активуючи функціональність відповідного вимикання вузла. Для активації цієї функції, два налаштування конфігурації kubelet повинні бути належним чином налаштовані та встановлені на значення, відмінні від нуля.

Якщо systemd виявляє або повідомляє про вимикання вузла, kubelet встановлює умову `NotReady` на вузлі з причиною `"node is shutting down"`. Kube-scheduler дотримується цієї умови та не планує жодних Podʼів на цьому вузлі; очікується, що інші планувальники сторонніх постачальників дотримуватимуться такої ж логіки. Це означає, що нові Podʼи не будуть плануватися на цьому вузлі, і, отже, жоден із них не розпочне роботу.

Kubelet **також** відхиляє Podʼи під час фази `PodAdmission`, якщо виявлено поточне
вимикання вузла, так що навіть Podʼи з {{< glossary_tooltip text="toleration" term_id="toleration" >}} для `node.kubernetes.io/not-ready:NoSchedule` не почнуть виконання там.

Водночас коли kubelet встановлює цю умову на своєму вузлі через API, kubelet також починає завершення будь-яких Podʼів, які виконуються локально.

Під час вимикання kubelet завершує Podʼи в два етапи:

1. Завершує звичайні Podʼи, які виконуються на вузлі.
2. Завершує [критичні Podʼи](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical) виконуються на вузлі.

Функцію відповідного вимикання вузла налаштовується двома параметрами конфігурації kubelet:

- `shutdownGracePeriod`:
  - Визначає загальний час, протягом якого вузол повинен затримати вимикання. Це загальний термін допомогає завершити Podʼи як звичайні, так і [критичні](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).
- `shutdownGracePeriodCriticalPods`:
  - Визначає термін, який використовується для завершення [критичних Podʼів](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical) під час вимикання вузла. Це значення повинно бути менше за `shutdownGracePeriod`.

{{< note >}}
Є випадки, коли вимкнення вузла було скасовано системою (або, можливо, вручну адміністратором). У будь-якому з цих випадків вузол повернеться до стану `Ready`.
Однак Podʼи, які вже розпочали процес завершення, не будуть відновлені kubelet
і їх потрібно буде перепланувати.
{{< /note >}}

Наприклад, якщо `shutdownGracePeriod=30s`, а `shutdownGracePeriodCriticalPods=10s`, kubelet затримає вимикання вузла на 30 секунд. Під час вимикання перші 20 (30-10) секунд будуть зарезервовані для відповідного завершення звичайних Podʼів, а останні 10 секунд будуть зарезервовані для завершення [критичних Podʼів](/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#marking-pod-as-critical).

{{< note >}}
Коли Podʼи були виведені під час відповідного вимикання вузла, вони позначаються як вимкнені. Виклик `kubectl get pods` показує стан виведених Podʼів як `Terminated`. І `kubectl describe pod` вказує, що Pod був виведений через вимикання вузла:

```none
Reason:         Terminated
Message:        Pod was terminated in response to imminent node shutdown.
```

{{< /note >}}

### Відповідне вимикання вузла на основі пріоритету Podʼа {#pod-priority-graceful-node-shutdown}

{{< feature-state feature_gate_name="GracefulNodeShutdownBasedOnPodPriority" >}}

Щоб забезпечити більше гнучкості під час відповідного вимикання вузла щодо порядку Podʼів під час вимикання, поступове вимикання вузла враховує PriorityClass для Podʼів, за умови, що ви активували цю функцію у своєму кластері. Функція дозволяє адміністраторам кластера явно визначити порядок Podʼів під час відповідного вимикання вузла на основі [priority classes](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass).

Функція [відповідного вимикання вузла](#graceful-node-shutdown), яка описана вище, вимикає Podʼи в дві фази: звичайні Podʼи, а потім критичні Podʼи. Якщо потрібна додаткова гнучкість для явного визначення порядку Podʼи під час вимикання в більш деталізований спосіб, можна використовувати поступове вимикання вузла на основі пріоритету Podʼа.

Коли вимикання вузла враховує пріоритет Podʼів, це дозволяє виконувати вимикання вузла у кілька етапів, кожен етап — це завершення роботи Podʼів певного класу пріоритету. Kubelet можна налаштувати з точним числом етапів та часом вимкнення для кожного етапу.

Припустимо, що в кластері існують наступні власні [класи пріоритету Podʼа](/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass):

|Назва класу пріоритету Podʼа|Значення класу пріоритету Podʼа|
|-------------------------|------------------------|
|`custom-class-a`         | 100000                 |
|`custom-class-b`         | 10000                  |
|`custom-class-c`         | 1000                   |
|`звичайний/не встановлено`          | 0                      |

В межах [конфігурації kubelet](/docs/reference/config-api/kubelet-config.v1beta1/) налаштування для `shutdownGracePeriodByPodPriority` може виглядати так:

|Значення класу пріоритету Podʼа|Період вимкнення|
|------------------------|---------------|
| 100000                 |10 seconds     |
| 10000                  |180 seconds    |
| 1000                   |120 seconds    |
| 0                      |60 seconds     |

Відповідна конфігурація YAML kubelet виглядатиме так:

```yaml
shutdownGracePeriodByPodPriority:
  - priority: 100000
    shutdownGracePeriodSeconds: 10
  - priority: 10000
    shutdownGracePeriodSeconds: 180
  - priority: 1000
    shutdownGracePeriodSeconds: 120
  - priority: 0
    shutdownGracePeriodSeconds: 60
```

Вищеописана таблиця означає, що будь-який Pod зі значенням `priority` >= 100000 отримає лише 10 секунд на зупинку, будь-який Pod зі значенням >= 10000 і < 100000 отримає 180 секунд для зупинки, будь-який Pod зі значенням >= 1000 і < 10000 отримає 120 секунд для зупинки. Нарешті, всі інші Podʼи отримають 60 секунд для зупинки.

Не обовʼязково вказувати значення, відповідні всім класам. Наприклад, можна використовувати ці налаштування:

|Значення класу пріоритету Podʼа|Період вимкнення|
|------------------------|---------------|
| 100000                 |300 seconds    |
| 1000                   |120 seconds    |
| 0                      |60 seconds     |

У випадку, коли в певному діапазоні пріоритету немає Podʼів, kubelet не чекатиме на Podʼи в тому діапазоні пріоритету. Замість цього kubelet негайно перейде до наступного діапазону значень класу пріоритету.

Якщо ця функція активована, і не надано конфігурацію, то жодної дії з порядком не буде вжито.

Використання цієї функції передбачає активацію [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `GracefulNodeShutdownBasedOnPodPriority`, і встановлення `ShutdownGracePeriodByPodPriority` в [kubelet config](/docs/reference/config-api/kubelet-config.v1beta1/) до потрібної конфігурації, яка містить значення класу пріоритету Podʼа та відповідні періоди вимкнення.

{{< note >}}
Здатність щоб враховувати пріоритет Podʼа під час відповідного вимикання вузла була введена як альфа-функція в Kubernetes v1.23. У Kubernetes {{< skew currentVersion >}} функція є бета-версією та є типово активованою.
{{< /note >}}

Метрики `graceful_shutdown_start_time_seconds` та `graceful_shutdown_end_time_seconds` виділяються під систему kubelet для моніторингу вимкнень вузлів.

## Обробка вимкнення вузла без використання відповідного вимкнення {#non-graceful-node-shutdown}

{{< feature-state feature_gate_name="NodeOutOfServiceVolumeDetach" >}}

Дія вимкнення вузла може не виявитися Node Shutdown Manager вузла kubelet, чи то через те, що команда не спричинює механізм блокування інгібітора, який використовується kubelet, чи через помилку користувача, тобто ShutdownGracePeriod і
ShutdownGracePeriodCriticalPods налаштовані неправильно. Будь ласка, зверніться до вищезазначеної секції [Відповідне вимкання вузла](#graceful-node-shutdown) для отримання докладнішої інформації.

Коли вузол вимикається, але не виявляється Node Shutdown Manager вузла kubelet, Podʼи, які є частиною {{< glossary_tooltip text="StatefulSet" term_id="statefulset" >}}, залишаться в стані завершення на вимкненому вузлі і не зможуть перейти до нового робочого вузла. Це тому, що kubelet на вимкненому вузлі недоступний для видалення Podʼів, і StatefulSet не може створити новий Pod із такою ж назвою. Якщо є томи, які використовуються Podʼами, то VolumeAttachments не буде видалено з оригінального вимкненого вузла, і тому томи використовувані цими Podʼами не можуть бути приєднані до нового робочого вузла. В результаті застосунок, що виконується на StatefulSet, не може працювати належним чином. Якщо оригінальний вимкнений вузол вмикається, Podʼи будуть видалені kubelet, і нові Podʼи будуть створені на іншому робочому вузлі. Якщо оригінальний вимкнений вузол не повертається, ці Podʼи залишаться в стані завершення на вимкненому вузлі назавжди.

Для помʼякшення вищезазначеної ситуації користувач може вручну додати позачку `node.kubernetes.io/out-of-service` із ефектом `NoExecute` чи `NoSchedule` до вузла, вказавши, що він вийшов із ладу. Якщо у {{< glossary_tooltip text="kube-controller-manager" term_id="kube-controller-manager" >}} увімкнено
[feature gate](/docs/reference/command-line-tools-reference/feature-gates/)
`NodeOutOfServiceVolumeDetach`, і вузол відзначений як такий, що вийшов з ладу з такою позначкою, Podʼи на вузлі будуть насильно видалені, якщо на них немає відповідних toleration, і операції відʼєднання томів для завершення Podʼів
на вузлі відбудуться негайно. Це дозволяє Podʼам на вузлі, що вийшов із строю, швидко відновитися на іншому вузлі.

Під час такого вимкання робота Podʼів завершується у дві фази:

1. Насильно видаляються Podʼи, які не мають відповідних toleration `out-of-service`.
2. Негайно виконується операція відʼєднання тома для таких Podʼів.

{{< note >}}

- Перш ніж додавати позначку `node.kubernetes.io/out-of-service`, слід перевірити, що вузол вже перебуває в стані припиення роботи чи вимкнення (не в середині перезавантаження).
- Користувач повинен вручну видалити позначку "вийшов із строю" після того, як podʼи будуть переміщені на новий вузол, і користувач перевірив, що вимкнений вузол відновився, оскільки саме користувач додав позначку на початку.
{{< /note >}}

## Керування swapʼом {#swap-memory}

{{< feature-state feature_gate_name="NodeSwap" >}}

Щоб увімкнути swap на вузлі, feature gate `NodeSwap` повинен бути активований на
kubelet, і прапорець командного рядка `--fail-swap-on` або [параметр конфігурації](/docs/reference/config-api/kubelet-config.v1beta1/)  `failSwapOn`  повинен бути встановлений в значення false.

{{< warning >}}
Коли увімкнено swap, дані Kubernetes, такі як вміст обʼєктів Secret, які були записані у tmpfs, тепер можуть переноситись на диск.
{{< /warning >}}

Користувач також може налаштувати `memorySwap.swapBehavior`, щоб вказати, як вузол буде використовувати swap. Наприклад,

```yaml
memorySwap:
  swapBehavior: UnlimitedSwap
```

- `UnlimitedSwap` (типово): Робочі навантаження Kubernetes можуть використовувати стільки swap, скільки вони запитують, до ліміту системи.
- `LimitedSwap`: Використання робочими навантаженнями Kubernetes swap обмежено. Тільки Podʼи Burstable QoS мають право використовувати swap.

Якщо конфігурація для `memorySwap` не вказана, і feature gate увімкнено, типово kubelet застосовує тe ж самe поведінкe, що й налаштування `UnlimitedSwap`.

З `LimitedSwap`, Podʼам, які не відносяться до класифікації Burstable QoS (тобто Podʼи QoS `BestEffort`/`Guaranteed`), заборонено використовувати swap. Для забезпечення гарантій безпеки і справності вузла цим Podʼам заборонено використовувати swap при включеному `LimitedSwap`.

Перед тим як розглядати обчислення ліміту swap, необхідно визначити наступне:

- `nodeTotalMemory`: Загальна кількість фізичної памʼяті, доступної на вузлі.
- `totalPodsSwapAvailable`: Загальний обсяг swap на вузлі, доступний для використання Podʼами (деякий swap може бути зарезервовано для системного використання).
- `containerMemoryRequest`: Запит на памʼять для контейнера.

Ліміт swap налаштовується так: `(containerMemoryRequest / nodeTotalMemory) * totalPodsSwapAvailable`.

Важливо враховувати, що для контейнерів у Podʼах Burstable QoS можна
відмовитися від використання swap, вказавши запити на памʼять, рівні лімітам памʼяті. Контейнери, налаштовані таким чином, не матимуть доступу до swap.

Swap підтримується тільки з **cgroup v2**, cgroup v1 не підтримується.

Для отримання додаткової інформації, а також для допомоги у тестуванні та надання зворотного звʼязку, будь ласка, перегляньте блог-пост [Kubernetes 1.28: NodeSwap виходить у Beta1](/blog/2023/08/24/swap-linux-beta/), [KEP-2400](https://github.com/kubernetes/enhancements/issues/4128) та його [проєкт концепції](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/2400-node-swap/README.md).

## {{% heading "whatsnext" %}}

Дізнайтеся більше про наступне:

- [Компоненти](/docs/concepts/overview/components/#node-components), з яких складається вузол.
- [Визначення API для вузла](/docs/reference/generated/kubernetes-api/{{< param "version" >}}/#node-v1-core).
- [Node](https://git.k8s.io/design-proposals-archive/architecture/architecture.md#the-kubernetes-node) у документі з дизайну архітектури.
- [Taints and Tolerations](/docs/concepts/scheduling-eviction/taint-and-toleration/).
- [Менеджери ресурсів вузла](/docs/concepts/policy/node-resource-managers/).
- [Управління ресурсами для вузлів з операційною системою Windows](/docs/concepts/configuration/windows-resource-management/).
