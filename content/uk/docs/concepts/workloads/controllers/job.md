---
reviewers:
- alculquicondor
- erictune
- soltysh
title: Jobs
content_type: concept
description: >-
  Job – є одноразовим завданням, що виконується до моменту його завершення.
feature:
  title: Пакетне виконання
  description: >
    На додачу до сервісів, Kubernetes може керувати пакетними та CI завданнями, замінюючи, якщо треба, контейнери, які зазнали збоїв.
weight: 50
hide_summary: true # Listed separately in section index
---

<!-- overview -->

Завдання (Job) створює один або кілька Podʼів і буде продовжувати повторювати виконання Podʼів, доки не буде досягнуто вказану кількість успішних завершень. При успішному завершенні Podʼів завдання відстежує ці успішні завершення. Коли досягнута вказана кількість успішних завершень, завдання (тобто Job) завершується. Видалення завдання буде видаляти Podʼи, які воно створило. При призупиненні завдання буде видаляти активні Podʼи, доки завдання знову не буде відновлене.

У простому випадку можна створити один обʼєкт Job для надійного запуску одного Podʼа до завершення. Обʼєкт Job буде запускати новий Pod, якщо перший Pod зазнає невдачі або видаляється (наприклад, через відмову апаратного забезпечення вузла або перезавантаження вузла).

Також можна використовувати Job для запуску кількох Podʼів паралельно.

Якщо ви хочете запустити завдання (або одне завдання, або декілька паралельно) за розкладом, див. [CronJob](/docs/concepts/workloads/controllers/cron-jobs/).

<!-- body -->

## Запуск прикладу Job {#running-an-example-job}

Ось конфігурація прикладу Job. Тут обчислюється число π з точністю до 2000 знаків і виконується його вивід. Виконання зазвичай займає близько 10 секунд.

{{% code_sample file="controllers/job.yaml" %}}

Ви можете запустити цей приклад за допомогою наступної команди:

```shell
kubectl apply -f https://kubernetes.io/examples/controllers/job.yaml
```

Вивід буде схожим на це:

```
job.batch/pi created
```

Перевірте статус Job за допомогою `kubectl`:

{{< tabs name="Check status of Job" >}}
{{< tab name="kubectl describe job pi" codelang="bash" >}}
Name:           pi
Namespace:      default
Selector:       batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
Labels:         batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
                batch.kubernetes.io/job-name=pi
                ...
Annotations:    batch.kubernetes.io/job-tracking: ""
Parallelism:    1
Completions:    1
Start Time:     Mon, 02 Dec 2019 15:20:11 +0200
Completed At:   Mon, 02 Dec 2019 15:21:16 +0200
Duration:       65s
Pods Statuses:  0 Running / 1 Succeeded / 0 Failed
Pod Template:
  Labels:  batch.kubernetes.io/controller-uid=c9948307-e56d-4b5d-8302-ae2d7b7da67c
           batch.kubernetes.io/job-name=pi
  Containers:
   pi:
    Image:      perl:5.34.0
    Port:       <none>
    Host Port:  <none>
    Command:
      perl
      -Mbignum=bpi
      -wle
      print bpi(2000)
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  21s   job-controller  Created pod: pi-xf9p4
  Normal  Completed         18s   job-controller  Job completed
{{< /tab >}}
{{< tab name="kubectl get job pi -o yaml" codelang="bash" >}}
apiVersion: batch/v1
kind: Job
metadata:
  annotations: batch.kubernetes.io/job-tracking: ""
             ...  
  creationTimestamp: "2022-11-10T17:53:53Z"
  generation: 1
  labels:
    batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
    batch.kubernetes.io/job-name: pi
  name: pi
  namespace: default
  resourceVersion: "4751"
  uid: 204fb678-040b-497f-9266-35ffa8716d14
spec:
  backoffLimit: 4
  completionMode: NonIndexed
  completions: 1
  parallelism: 1
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
  suspend: false
  template:
    metadata:
      creationTimestamp: null
      labels:
        batch.kubernetes.io/controller-uid: 863452e6-270d-420e-9b94-53a54146c223
        batch.kubernetes.io/job-name: pi
    spec:
      containers:
      - command:
        - perl
        - -Mbignum=bpi
        - -wle
        - print bpi(2000)
        image: perl:5.34.0
        imagePullPolicy: IfNotPresent
        name: pi
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
status:
  active: 1
  ready: 0
  startTime: "2022-11-10T17:53:57Z"
  uncountedTerminatedPods: {}
{{< /tab >}}
{{< /tabs >}}

Щоб переглянути завершені Podʼи Job, використовуйте `kubectl get pods`.

Щоб вивести всі Podʼи, які належать Job у машинночитаній формі, ви можете використовувати таку команду:

```shell
pods=$(kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}')
echo $pods
```

Вивід буде схожим на це:

```none
pi-5rwd7
```

Тут, селектор збігається з селектором, який використовується в Job. Параметр `--output=jsonpath` зазначає вираз з назвою з кожного Pod зі списку.

```shell
kubectl logs $pods
```

Іншим варіантом є використання `kubectl logs` для виводу логів з кожного Pod.

```shell
kubectl logs job/pi
```

Вивід буде схожим на це:

```none
3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901
```

### Створення опису Job {#writing-a-job-spec}

Як і з будь-якою іншою конфігурацією Kubernetes, у Job мають бути вказані поля `apiVersion`, `kind` та `metadata`.

При створенні панеллю управління нових Podʼів для Job, поле `.metadata.name` Job є частиною основи для надання імен цим Podʼам. Імʼя Job повинно бути дійсним значенням [DNS-піддомену](/docs/concepts/overview/working-with-objects/names#dns-subdomain-names), але це може призводити до неочікуваних результатів для імен хостів Podʼів. Для найкращої сумісності імʼя повинно відповідати більш обмеженим правилам для [DNS-мітки](/docs/concepts/overview/working-with-objects/names#dns-label-names). Навіть якщо імʼя є DNS-піддоменом, імʼя повинно бути не довше 63 символів.

Також у Job повинен бути розділ [`.spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status).

### Мітки для Job {#job-labels}

Мітки Job повинні мати префікс `batch.kubernetes.io/` для `job-name` та `controller-uid`.

### Шаблон Podʼа {#pod-template}

`.spec.template` — єдине обовʼязкове поле `.spec`.

`.spec.template` — це [шаблон Podʼа](/docs/concepts/workloads/pods/#pod-templates). Він має точно таку ж схему, як {{< glossary_tooltip text="Pod" term_id="pod" >}}, окрім того, що він вкладений і не має `apiVersion` чи `kind`.

Окрім обовʼязкових полів для Podʼа , шаблон Podʼа  в Job повинен вказати відповідні мітки (див. [вибір Podʼа](#pod-selector)) та відповідну політику перезапуску.

Дозволяється лише [`RestartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy), рівний `Never` або `OnFailure`.

### Селектор Podʼа {#pod-selector}

Поле `.spec.selector` є необовʼязковим. У майже всіх випадках ви не повинні його вказувати. Дивіться розділ [вказування вашого власного селектора Podʼа](#specifying-your-own-pod-selector).

### Паралельне виконання для Jobs {#parallel-jobs}

Існують три основних типи завдань, які підходять для виконання в якості Job:

1. Непаралельні Jobs
   - зазвичай запускається лише один Pod, якщо Pod не вдається.
   - Job завершується, як тільки його Pod завершується успішно.

1. Паралельні Jobs із *фіксованою кількістю завершень*
   - вкажіть ненульове позитивне значення для `.spec.completions`.
   - Job являє собою загальне завдання і завершується, коли є `.spec.completions` успішних Podʼів.
   - при використанні `.spec.completionMode="Indexed"`, кожен Pod отримує різний індекс у діапазоні від 0 до `.spec.completions-1`.

1. Паралельні Jobs із *чергою завдань*
   - не вказуйте `.spec.completions`, типово встановлюється `.spec.parallelism`.
   - Podʼи повинні координувати серед себе або зовнішній сервіс для визначення над чим кожен з них має працювати. Наприклад, Pod може отримати набір з N елементів з черги завдань.
   - кожен Pod може незалежно визначити, чи завершилися всі його партнери, і, таким чином, що весь Job завершений.
   - коли *будь-який* Pod з Job завершується успішно, нові Podʼи не створюються.
   - як тільки хоча б один Pod завершився успішно і всі Podʼи завершені, тоді Job завершується успішно.
   - як тільки будь-який Pod завершився успішно, жоден інший Pod не повинен виконувати роботу для цього завдання чи записувати будь-який вивід. Вони всі мають бути в процесі завершення.

Для *непаралельного* Job ви можете залишити як `.spec.completions`, так і `.spec.parallelism` не встановленими. Коли обидва не встановлені, обидва типово встановлюються на 1.

Для *Job із фіксованою кількістю завершень* вам слід встановити `.spec.completions` на кількість необхідних завершень. Ви можете встановити `.spec.parallelism` чи залишити його не встановленим, і він буде встановлено типово на 1.

Для *Job із чергою завдань* ви повинні залишити `.spec.completions` не встановленим і встановити `.spec.parallelism` на не відʼємне ціле число.

За докладною інформацією про використання різних типів Job, див. розділ [шаблони Job](#job-templates).

#### Контроль паралелізму {#controlling-parallelism}

Запитаний паралелізм (`.spec.parallelism`) може бути встановлений на будь-яке не відʼємне значення. Якщо значення не вказано, стандартно воно дорівнює 1. Якщо вказано як 0, то Job ефективно призупинено до тих пір, поки воно не буде збільшене.

Фактична паралельність (кількість Podʼів, які працюють в будь-який момент) може бути більше чи менше, ніж запитаний паралелізм, з різних причин:

- Для *Job із фіксованою кількістю завершень*, фактична кількість Podʼів, які працюють паралельно, не буде перевищувати кількість залишених завершень. Значення `.spec.parallelism` більше чи менше ігнорується.
- Для *Job із чергою завдань*, жоден новий Pod не запускається після успішного завершення будь-якого Podʼа — залишені Podʼи мають право завершити роботу.
- Якщо у {{< glossary_tooltip term_id="controller" text="контролера" >}} Job не було часу відреагувати.
- Якщо контролер Job не зміг створити Podʼи з будь-якої причини (нестача `ResourceQuota`, відсутність дозволу і т.д.), тоді може бути менше Podʼів, ніж запитано.
- Контролер Job може обмежувати створення нових Podʼів через ексцесивні попередні відмови Podʼів у цьому ж Job.
- Коли Pod завершується відповідним чином, на його зупинку потрібен час.

### Режим завершення {#completion-mode}

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

Для завдань з *фіксованою кількістю завершень* — тобто завдань, які мають не нульове `.spec.completions` — може бути встановлено режим завершення, який вказується в `.spec.completionMode`:

- `NonIndexed` (стандартно): завдання вважається завершеним, коли є `.spec.completions` успішно завершених Podʼів. Іншими словами, завершення кожного Podʼа гомологічне одне одному. Зверніть увагу, що завдання, у яких `.spec.completions` дорівнює null, неявно є `NonIndexed`.
- `Indexed`: Podʼи завдання отримують асоційований індекс завершення від 0 до `.spec.completions-1`. Індекс доступний через чотири механізми:
  - Анотація Podʼа `batch.kubernetes.io/job-completion-index`.
  - Мітка Podʼа `batch.kubernetes.io/job-completion-index` (для v1.28 і новіших). Зверніть увагу, що для використання цієї мітки механізм feature gate `PodIndexLabel` повинен бути увімкнений, і він є типово увімкненим.
  - Як частина імені хоста Podʼа, за шаблоном `$(job-name)-$(index)`. Коли ви використовуєте індексоване завдання у поєднанні з {{< glossary_tooltip term_id="Service" >}}, Podʼи всередині завдання можуть використовувати детерміністичні імена хостів для адресації одне одного через DNS. Докладні відомості щодо того, як налаштувати це, див. [Завдання з комунікацією від Podʼа до Podʼа](/docs/tasks/job/job-with-pod-to-pod-communication/).
  - З контейнера завдання, в змінній середовища `JOB_COMPLETION_INDEX`.
  
  Завдання вважається завершеним, коли є успішно завершений Pod для кожного індексу. Докладні відомості щодо того, як використовувати цей режим, див. [Індексоване завдання для паралельної обробки зі статичним призначенням роботи](/docs/tasks/job/indexed-parallel-processing-static/).

{{< note >}}
Хоча це і рідко, може бути запущено більше одного Podʼа для одного і того ж індексу (з різних причин, таких як відмови вузла, перезапуски kubelet чи витіснення Podʼа). У цьому випадку лише перший успішно завершений Pod буде враховуватися при підрахунку кількості завершень та оновленні статусу завдання. Інші Podʼи, які працюють чи завершили роботу для того ж самого індексу, будуть видалені контролером завдання, як тільки він їх виявлять.
{{< /note >}}

## Обробка відмов Podʼа та контейнера {#handling-pod-and-container-failures}

Контейнер у Podʼі може вийти з ладу з ряду причин, таких як завершення процесу з ненульовим кодом виходу або примусове припинення роботи контейнера через перевищення ліміту памʼяті та таке інше. Якщо це стається і `.spec.template.spec.restartPolicy = "OnFailure"`, тоді Pod залишається на вузлі, але контейнер перезапускається. Отже, ваш застосунок повинен обробляти випадок, коли він
перезапускається локально, або вказувати `.spec.template.spec.restartPolicy = "Never"`. Докладні відомості про `restartPolicy` див. в розділі [життєвий цикл Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/#example-states).

Весь Pod також може вийти з ладу з ряду причин, таких як коли Pod видаляється з  вузла (вузол оновлюється, перезавантажується, видаляється тощо), або якщо контейнер Podʼа  виходить з ладу і `.spec.template.spec.restartPolicy = "Never"`. Коли Pod виходить з ладу, контролер  завдання запускає новий Pod. Це означає, що ваш застосунок повинен обробляти випадок, коли він перезапускається у новому
Podʼі. Зокрема, він повинен обробляти тимчасові файли, блокування, неповний вивід та інше, викликане попередніми запусками.

Типово кожна відмова Podʼа враховується в ліміті `.spec.backoffLimit`, див. [політика відмови Podʼа](#pod-backoff-failure-policy). Однак ви можете налаштовувати обробку відмов Podʼа, встановлюючи [політику відмови Podʼа] (#pod-failure-policy) для завдання.

Додатково ви можете вирішити враховувати відмови Podʼа незалежно для кожного індексу в [Індексованому](#completion-mode) завданні, встановлюючи поле `.spec.backoffLimitPerIndex` (докладні відомості див. [ліміт затримки на індекс](#backoff-limit-per-index)).

Зверніть увагу, що навіть якщо ви вказали `.spec.parallelism = 1` і `.spec.completions = 1` і `.spec.template.spec.restartPolicy = "Never"`, той самий програмний код може іноді бути запущений двічі.

Якщо ви вказали як `.spec.parallelism`, так і `.spec.completions` більше ніж 1, то може бути запущено кілька Podʼів одночасно. Таким чином, ваші Podʼи також повинні бути стійкими до конкурентності.

Коли вмикаються [feature gates](/docs/reference/command-line-tools-reference/feature-gates/) `PodDisruptionConditions` та `JobPodFailurePolicy`, і поле `.spec.podFailurePolicy` встановлено, контролер завдання не вважає завершення
Podʼа (Pod, який має встановлене поле `.metadata.deletionTimestamp`) за відмову, поки цей Pod не є термінальним (його `.status.phase` — `Failed` або `Succeeded`). Однак контролер завдання створює новий Pod, як тільки стає очевидним завершення. Якщо Pod завершується, контролер завдання оцінює `.backoffLimit` та `.podFailurePolicy` для відповідного завдання, враховуючи цей тепер вже завершений Pod.

Якщо хоча б одна з цих вимог не виконана, контролер завдання рахує завершення Podʼа негайною відмовою, навіть якщо цей Pod пізніше завершить  фазою `"Succeeded"`.

### Політика відмови затримки Podʼа {#pod-backoff-failure-policy}

Є ситуації, коли ви хочете відмовити завдання після певної кількості спроб
через логічну помилку в конфігурації тощо. Для цього встановіть `.spec.backoffLimit`, щоб вказати кількість спроб перед тим, як вважати завданням невдалим. Ліміт затримки стандартно встановлено на рівні 6. Помилкові Podʼи, повʼязані з завданням, відновлюються контролером завдань з експоненційною затримкою (10 с, 20 с, 40 с …) з верхнім обмеженням у шість хвилин.

Кількість спроб обчислюється двома способами:

- Кількість Podʼів з `.status.phase = "Failed"`.
- При використанні `restartPolicy = "OnFailure"` кількість спроб у всіх   контейнерах Podʼів з `.status.phase`, що дорівнює `Pending` або `Running`.

Якщо хоча б один із розрахунків досягне значення `.spec.backoffLimit`, завдання вважається невдалим.

{{< note >}}
Якщо ваше завдання має `restartPolicy = "OnFailure"`, майте на увазі, що Pod, який виконує завдання, буде припинений, як тільки ліміт затримки завдання буде досягнуто. Це може ускладнити налагодження виконуваного завдання. Ми радимо встановлювати `restartPolicy = "Never"` під час налагодження завдання або використовувати систему логування, щоб забезпечити, що вивід з невдалого завдання не буде втрачено випадково.
{{< /note >}}

### Ліміт затримки для кожного індексу {#backoff-limit-per-index}

{{< feature-state for_k8s_version="v1.29" state="beta" >}}

{{< note >}}
Ви можете налаштувати ліміт затримки для кожного індексу для завдання з [індексацією](#completion-mode), якщо у вас увімкнено [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `JobBackoffLimitPerIndex` в вашому кластері.
{{< /note >}}

Коли ви запускаєте [індексоване](#completion-mode) завдання, ви можете вибрати обробку спроб для відмов Podʼа незалежно для кожного індексу. Для цього встановіть
`.spec.backoffLimitPerIndex`, щоб вказати максимальну кількість невдалих спроб Podʼа
для кожного індексу.

Коли ліміт затримки для кожного індексу перевищується для певного індексу, Kubernetes вважає цей індекс невдалим і додає його до поля `.status.failedIndexes`. Індекси, які виконались успішно, реєструються в полі `.status.completedIndexes`, незалежно від того, чи ви встановили поле `backoffLimitPerIndex`.

Зауважте, що невдалий індекс не перериває виконання інших індексів. Щойно всі індекси завершаться для завдання, в якому ви вказали ліміт затримки для кожного індексу, якщо хоча б один з цих індексів виявився невдалим, контролер завдань позначає загальне завдання як невдале, встановлюючи умову `Failed` в статусі. Завдання отримує позначку "невдало", навіть якщо деякі, можливо, усі індекси були
оброблені успішно.

Ви також можете обмежити максимальну кількість позначених невдалих індексів, встановивши поле `.spec.maxFailedIndexes`. Коли кількість невдалих індексів перевищує значення поля `maxFailedIndexes`, контролер завдань запускає завершення всіх залишаються запущеними Podʼами для цього завдання. Щойно всі Podʼи будуть завершені, контролер завдань позначає всю роботу як невдалу, встановлюючи умову `Failed` в статусі завдання.

Ось приклад маніфесту для завдання, яке визначає `backoffLimitPerIndex`:

{{< code_sample file="/controllers/job-backoff-limit-per-index-example.yaml" >}}

У вищенаведеному прикладі контролер завдань дозволяє один перезапуск для кожного з індексів. Коли загальна кількість невдалих індексів перевищує 5, тоді все завдання припиняються.

Після завершення роботи стан завдання виглядає наступним чином:

```sh
kubectl get -o yaml job job-backoff-limit-per-index-example
```

```yaml
  status:
    completedIndexes: 1,3,5,7,9
    failedIndexes: 0,2,4,6,8
    succeeded: 5          # 1 succeeded pod for each of 5 succeeded indexes
    failed: 10            # 2 failed pods (1 retry) for each of 5 failed indexes
    conditions:
    - message: Job has failed indexes
      reason: FailedIndexes
      status: "True"
      type: Failed
```

Додатково ви можете використовувати ліміт затримки для кожного індексу разом з
[політикою збоїв Podʼа](#pod-failure-policy). Коли використовується ліміт затримки для кожного індексу, доступний новій дії `FailIndex`, який дозволяє вам уникати непотрібних повторів всередині індексу.

### Політика збою Podʼа {#pod-failure-policy}

{{< feature-state for_k8s_version="v1.26" state="beta" >}}

{{< note >}}
Ви можете налаштувати політику збою Podʼа для завдання лише тоді, коли [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `JobPodFailurePolicy` увімкнено у вашому кластері. Крім того, рекомендується увімкнути feature gate `PodDisruptionConditions`, щоб мати можливість виявляти та обробляти умови розладу Podʼа в політиці збою Podʼа (див. також: [умови розладу Podʼа](/docs/concepts/workloads/pods/disruptions#pod-disruption-conditions)). Обидва feature gate доступні в Kubernetes {{< skew currentVersion >}}.
{{< /note >}}

Політика збою Podʼа, визначена за допомогою поля `.spec.podFailurePolicy`, дозволяє вашому кластеру обробляти відмови Podʼа на основі кодів виходу контейнера та умов Podʼа.

У деяких ситуаціях вам може знадобитися кращий контроль обробки відмов Podʼа, ніж контроль, який надає [політика збою Podʼа за допомогою затримки збою Podʼа](#pod-backoff-failure-policy), яка базується на `.spec.backoffLimit` завдання. Ось деякі приклади використання:

- Для оптимізації витрат на виконання завдань, уникнення непотрібних перезапусків Podʼа, ви можете завершити завдання, як тільки один із його Podʼів відмовить із кодом виходу, що вказує на помилку програмного забезпечення.
- Щоб гарантувати, що ваше завдання завершиться, навіть якщо є розлади, ви можете ігнорувати відмови Podʼа, спричинені розладами (такими як {{< glossary_tooltip text="витіснення" term_id="preemption" >}}, {{< glossary_tooltip text="ініційоване API вивільнення ресурсів" term_id="api-eviction" >}} або вивільнення на підставі {{< glossary_tooltip text="маркування" term_id="taint" >}}), щоб вони не враховувалися при досягненні `.spec.backoffLimit` ліміту спроб.

Ви можете налаштувати політику збою Podʼа в полі `.spec.podFailurePolicy`, щоб відповідати вищенаведеним використанням. Ця політика може обробляти відмови Podʼа на основі кодів виходу контейнера та умов Podʼа.

Ось маніфест для завдання, яке визначає `podFailurePolicy`:

{{% code_sample file="/controllers/job-pod-failure-policy-example.yaml" %}}

У вищенаведеному прикладі перше правило політики збою Podʼа вказує, що завдання слід позначити як невдале, якщо контейнер `main` завершиться кодом виходу 42. Наступні правила стосуються саме контейнера `main`:

- код виходу 0 означає, що контейнер виконався успішно
- код виходу 42 означає, що **все завдання** виконалося невдало
- будь-який інший код виходу вказує, що контейнер виконався невдало, і, отже, весь Pod буде створений заново, якщо загальна кількість перезапусків менше `backoffLimit`. Якщо `backoffLimit` досягнуте, **все завдання** виконалося невдало.

{{< note >}}
Оскільки в шаблоні Podʼа вказано `restartPolicy: Never`, kubelet не перезапускає контейнер `main` в тому конкретному Podʼі.
{{< /note >}}

Друге правило політики збою Podʼа, що вказує дію `Ignore` для невдалих Podʼів з умовою `DisruptionTarget`, виключає Podʼи, які взяли участь в розладах, з хунок ліміту спроб `.spec.backoffLimit`.

{{< note >}}
Якщо завдання виявилося невдалим, будь-то через політику збою Podʼа, чи через політику затримки збою Podʼа, і завдання виконується декількома Podʼами, Kubernetes завершує всі
Podʼи в цьому завданні, які все ще перебувають у статусах Pending або Running.
{{< /note >}}

Ось деякі вимоги та семантика API:

- якщо ви хочете використовувати поле `.spec.podFailurePolicy` для завдання Job, вам також слід визначити шаблон Podʼа цього завдання з `.spec.restartPolicy`, встановленим на `Never`.
- правила політики збою Podʼа, які ви визначаєте у `spec.podFailurePolicy.rules`, оцінюються послідовно. Якщо одне з правил відповідає збою Podʼа, інші правила ігноруються. Якщо жодне правило не відповідає збою Podʼа, застосовується типова обробка.
- ви можете бажати обмежити правило певним контейнером, вказавши його ім'я у `spec.podFailurePolicy.rules[*].onExitCodes.containerName`. Якщо не вказано, правило застосовується до всіх контейнерів. Якщо вказано, воно повинно відповідати імені одного з контейнерів або `initContainer` у шаблоні Podʼа.
- ви можете вказати дію, вживану при відповідності політиці збою Podʼа, у `spec.podFailurePolicy.rules[*].action`. Можливі значення:
  - `FailJob`: використовуйте це, щоб вказати, що завдання Podʼа має бути позначено як Failed та всі запущені Podʼи повинні бути завершені.
  - `Ignore`: використовуйте це, щоб вказати, що лічильник до `.spec.backoffLimit` не повинен збільшуватися, і повинен бути створений замінюючий Pod.
  - `Count`: використовуйте це, щоб вказати, що Pod повинен бути оброблений типовим способом. Лічильник до `.spec.backoffLimit` повинен збільшитися.
  - `FailIndex`: використовуйте цю дію разом із [лімітом затримки на кожен індекс](#backoff-limit-per-index) для уникнення непотрібних повторних спроб в межах індексу невдалого Podʼа.

{{< note >}}
Коли використовується `podFailurePolicy`, контролер завдань враховує лише Podʼи у фазі `Failed`. Podʼи з відміткою про видалення, які не знаходяться в термінальній фазі (`Failed` чи `Succeeded`), вважаються таким що примусово припиняють свою роботу. Це означає, що такі Podʼи зберігають [завершувачі відстеження](#job-tracking-with-finalizers) доки не досягнуть термінальної фази. З Kubernetes 1.27 Kubelet переводить видалені Podʼи в термінальну фазу (див.: [Фаза Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/#pod-phase)). Це забезпечує видалення завершувачів контролером Job.
{{< /note >}}

{{< note >}}
Починаючи з Kubernetes v1.28, коли використовується політика збою Podʼа, контролер Job відтворює Podʼи, що припиняються примусово, лише тоді, коли ці Podʼи досягли термінальної фази `Failed`. Це подібно до політики заміщення Podʼа: `podReplacementPolicy: Failed`. Для отримання додаткової інформації див. [Політика заміщення Podʼа](#pod-replacement-policy).
{{< /note >}}

## Завершення завдання та очищення {#job-termination-and-cleanup}

Коли завдання завершується, Podʼи більше не створюються, але Podʼи [зазвичай](#pod-backoff-failure-policy) також не видаляються. Тримання їх допомагає переглядати логи завершених Podʼів для перевірки наявності помилок, попереджень чи іншого діагностичного виводу. Обʼєкт завдання також залишається після завершення, щоб ви могли переглядати його статус. Користувачу слід видаляти старі завдання після перегляду їх статусу. Видаліть завдання за допомогою `kubectl` (наприклад, `kubectl delete jobs/pi` або `kubectl delete -f ./job.yaml`). Коли ви видаляєте завдання за допомогою `kubectl`, всі його створені Podʼи також видаляються.

Стандартно завдання буде виконуватися без перерви, поки Pod не вийде з ладу (`restartPolicy=Never`) або контейнер не вийде з ладу з помилкою (`restartPolicy=OnFailure`), після чого завдання дотримується `.spec.backoffLimit`, описаного вище. Як тільки досягнуто `.spec.backoffLimit`, завдання буде позначене як невдале, і будь-які запущені Podʼи будуть завершені.

Іншим способом завершити завдання є встановлення граничного терміну виконання. Це робиться шляхом встановлення поля `.spec.activeDeadlineSeconds` завдання кількості секунд. `activeDeadlineSeconds` застосовується до тривалості завдання, незалежно від кількості створених Podʼів. Як тільки Job досягає `activeDeadlineSeconds`, всі його запущені Podʼи завершуються, і статус завдання стане `type: Failed` з `reason: DeadlineExceeded`.

Зауважте, що `.spec.activeDeadlineSeconds` Job має пріоритет перед його `.spec.backoffLimit`. Отже, завдання, яке повторює один або кілька невдал х Podʼів, не розпочне розгортання додаткових Podʼів, якщо `activeDeadlineSeconds` вже досягнуто, навіть якщо `backoffLimit` не досягнуто.  Приклад:

```yaml
apiVersion: batch/v1 kind: Job
metadata:
  name: pi-with-timeout
Стандартноimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

Зверніть увагу, що як саме специфікація Job, так і [специфікація шаблону Pod](/docs/concepts/workloads/pods/init-containers/#detailed-behavior) у межах завдання мають поле `activeDeadlineSeconds`. Переконайтеся, що ви встановлюєте це поле на відповідному рівні.

Памʼятайте, що `restartPolicy` застосовується до Podʼа, а не до самого завдання: автоматичного перезапуску завдання не відбудеться, якщо статус завдання `type: Failed`. Іншими словами, механізми завершення завдання, активовані за допомогою `.spec.activeDeadlineSeconds` і `.spec.backoffLimit`, призводять до постійної невдачі завдання, що вимагає ручного втручання для вирішення.

## Автоматичне очищення завершених завдань {#clean-up-finished-jobs-automatically}

Зазвичай завершені завданя вже не потрібні в системі. Зберігання їх в системі може створювати тиск на сервер API. Якщо завдання керується безпосередньо контролером вищого рівня, таким як [CronJobs](/docs/concepts/workloads/controllers/cron-jobs/), завдання можна очищати за допомогою CronJobs на основі визначеної політики очищення з урахуванням місткості.

### Механізм TTL для завершених завдань {#ttl-mechanism-for-finished-jobs}

{{< feature-state for_k8s_version="v1.23" state="stable" >}}

Ще один спосіб автоматично очищати завершені завдання (якщо вони `Complete` або `Failed`) — це використовувати механізм TTL, наданий
[контролером TTL](/docs/concepts/workloads/controllers/ttlafterfinished/) для завершених ресурсів, вказуючи поле `.spec.ttlSecondsAfterFinished` у Job.

Коли контролер TTL очищує Job, він каскадно видаляє Job, тобто видаляє його залежні обʼєкти, такі як Podʼи, разом з Job. Зверніть увагу що при видаленні Job будуть дотримані гарантії його життєвого циклу, такі як завершувачі.

Наприклад:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl:5.34.0
        command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

Job `pi-with-ttl` буде призначено для автоматичного видалення через `100` секунд після завершення.

Якщо поле встановлено в `0`, Job буде призначено для автоматичного видалення негайно після завершення. Якщо поле не вказане, це Job не буде очищено контролером TTL після завершення.

{{< note >}}
Рекомендується встановлювати поле `ttlSecondsAfterFinished`, оскільки завдання без нагляду (завдання, які ви створили безпосередньо, а не опосередковано через інші API робочого навантаження такі як CronJob) мають станадартну політику видалення `orphanDependents`, що призводить до того, що Podʼи, створені некерованим Job, залишаються після того, як Job повністю видалено. Навіть якщо {{< glossary_tooltip text="панель управління" term_id="control-plane" >}} в кінцевому рахунку [збирає сміття](/docs/concepts/workloads/pods/pod-lifecycle/#pod-garbage-collection) Podʼів з видаленого Job після того, як вони або не вдались, або завершились, іноді ці залишки Podʼів можуть призводити до погіршання продуктивності кластера або в найгіршому випадку можуть спричинити відключення кластера через це погіршення.

Ви можете використовувати [LimitRanges](/docs/concepts/policy/limit-range/) та [ResourceQuotas](/docs/concepts/policy/resource-quotas/) щоб обмежити кількість ресурсів, які певний простір імен може споживати.
{{< /note >}}

## Патерни використання завдань (Job) {#job-patterns}

Обʼєкт завдання (Job) може використовуватися для обробки набору незалежних, але повʼязаних *робочих одиниць*. Це можуть бути електронні листи для надсилання, кадри для відтворення, файли для транскодування, діапазони ключів у базі даних NoSQL для сканування і так далі.

У складній системі може бути кілька різних наборів робочих одиниць. Тут ми розглядаємо лише один набір робочих одиниць, якими користувач хоче управляти разом — *пакетне завдання*.

Існує кілька різних патернів для паралельних обчислень, кожен з власними перевагами та недоліками. Компроміси:

- Один обʼєкт Job для кожної робочої одиниці або один обʼєкт Job для всіх робочих одиниць. Один Job на кожну робочу одиницю створює деяку накладну роботу для користувача та системи при управлінні великою кількістю об'єктів Job. Один Job для всіх робочих одиниць підходить краще для великої кількості одиниць.
- Кількість створених Podʼів дорівнює кількості робочих одиниць або кожен Pod може обробляти кілька робочих одиниць. Коли кількість Podʼів дорівнює кількості робочих одиниць, Podʼи зазвичай вимагають менше змін у наявному коді та контейнерах. Кожен Pod, що обробляє кілька робочих одиниць, підходить краще для великої кількості одиниць.
- Декілька підходів використовують робочу чергу. Це вимагає запуску служби черги та модифікацій існуючої програми чи контейнера, щоб зробити його сумісним із чергою роботи. Інші підходи легше адаптувати до наявного контейнеризованого застосунку.
- Коли Job повʼязаний із [headless Service](/docs/concepts/services-networking/service/#headless-services), ви можете дозволити Podʼам у межах Job спілкуватися один з одним для спільної обчислень.

Переваги та недоліки узагальнено у таблиці нижче, де стовпці з 2 по 4 відповідають зазначеним вище питанням. Імена патернів також є посиланнями на приклади та більш детальний опис.

| Патерн                                        | Один обʼєкт Job | Кількість Pods менше за робочі одиниці? | Використовувати застосунок без модифікацій? |
| ----------------------------------------------- |:-----------------:|:---------------------------:|:-------------------:|
| [Черга з Pod на одиницю роботи]                 |         ✓         |                             |      іноді           |
| [Черга змінної кількості Pod]                   |         ✓         |             ✓               |                     |
| [Індексована робота із статичним призначенням роботи]       |         ✓         |                                          |          ✓          |
| [Робота із спілкуванням від Pod до Pod]             |         ✓         |         іноді              |      іноді           |
| [Розширення шаблону роботи]                        |                   |                             |          ✓          |

Коли ви вказуєте завершення з `.spec.completions`, кожний Pod, створений контролером Job, має ідентичний [`spec`](https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status). Це означає, що всі Podʼи для завдання матимуть однакову командну строку та один і той же шаблон, та (майже) ті ж самі змінні середовища. Ці патерни — це різні способи організації Podʼів для роботи над різними завданнями.

Ця таблиця показує обовʼязкові налаштування для `.spec.parallelism` та `.spec.completions` для кожного з патернів. Тут `W` — це кількість робочих одиниць.

|             Патерн                             | `.spec.completions` |  `.spec.parallelism` |
| ----------------------------------------------- |:-------------------:|:--------------------:|
| [Черга з Pod на одиницю роботи]                 |          W          |        будь-яка        |
| [Черга змінної кількості Pod]                   |         null        |        будь-яка        |
| [Індексована робота із статичним призначенням роботи]       |          W          |        будь-яка        |
| [Робота із спілкуванням від Pod до Pod]             |          W          |         W            |
| [Розширення шаблону роботи]                        |          1          |     повинно бути 1   |

[Черга з Pod на одиницю роботи]: /docs/tasks/job/coarse-parallel-processing-work-queue/
[Черга змінної кількості Pod]: /docs/tasks/job/fine-parallel-processing-work-queue/
[Індексована робота із статичним призначенням роботи]: /docs/tasks/job/indexed-parallel-processing-static/
[Робота із спілкуванням від Pod до Pod]: /docs/tasks/job/job-with-pod-to-pod-communication/
[Розширення шаблону роботи]: /docs/tasks/job/parallel-processing-expansion/

## Розширене використання завдань (Job) {#advanced-usage}

### Відкладення завдання {#suspending-a-job}

{{< feature-state for_k8s_version="v1.24" state="stable" >}}

Коли створюється Job, контролер Job негайно починає створювати Podʼи, щоб відповісти вимогам Job і продовжує це робити, доки Job не завершиться. Однак іноді ви можете хотіти тимчасово призупинити виконання Job та відновити його пізніше, або створити Jobs у призупиненому стані та мати власний контролер, який вирішить, коли їх запустити.

Щоб призупинити Job, ви можете оновити поле `.spec.suspend` Job на значення true; пізніше, коли ви захочете відновити його, оновіть його на значення false. Створення Job з `.spec.suspend` встановленим в true створить його в призупиненому стані.

При відновленні Job з призупинення йому буде встановлено час початку `.status.startTime` в поточний час. Це означає, що таймер `.spec.activeDeadlineSeconds` буде зупинений і перезапущений, коли Job буде призупинено та відновлено.

Коли ви призупиняєте Job, будь-які запущені Podʼи, які не мають статусу `Completed`, будуть [завершені](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination) з сигналом SIGTERM. Буде дотримано період відповідного завершення ваших Podʼів, і вашим Podʼам слід обробити цей сигнал протягом цього періоду. Це може включати в себе збереження прогресу для майбутнього або скасування змін. Podʼи, завершені цим чином, не враховуватимуться при підрахунку `completions` Job.

Приклад визначення Job в призупиненому стані може виглядати так:

```shell
kubectl get job myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: myjob
spec:
  suspend: true
  parallelism: 1
  completions: 5
  template:
    spec:
      ...
```

Ви також можете перемикати призупинення Job, використовуючи командний рядок.

Призупиніть активний Job:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":true}}'
```

Відновіть призупинений Job:

```shell
kubectl patch job/myjob --type=strategic --patch '{"spec":{"suspend":false}}'
```

Статус Job може бути використаний для визначення того, чи Job призупинено чи його було зупинено раніше:

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata and .spec пропущено
status:
  conditions:
  - lastProbeTime: "2021-02-05T13:14:33Z"
    lastTransitionTime: "2021-02-05T13:14:33Z"
    status: "True"
    type: Suspended
  startTime: "2021-02-05T13:13:48Z"
```

Умова Job типу "Suspended" зі статусом "True" означає, що Job призупинено; поле `lastTransitionTime` може бути використане для визначення того, як довго Job було призупинено. Якщо статус цієї умови є "False", то Job було раніше призупинено і зараз працює. Якщо такої умови не існує в статусі Job, то Job ніколи не був зупинений.

Також створюються події, коли Job призупинено та відновлено:

```shell
kubectl describe jobs/myjob
```

```none
Name:           myjob
...
Events:
  Type    Reason            Age   From            Message
  ----    ------            ----  ----            -------
  Normal  SuccessfulCreate  12m   job-controller  Created pod: myjob-hlrpl
  Normal  SuccessfulDelete  11m   job-controller  Deleted pod: myjob-hlrpl
  Normal  Suspended         11m   job-controller  Job suspended
  Normal  SuccessfulCreate  3s    job-controller  Created pod: myjob-jvb44
  Normal  Resumed           3s    job-controller  Job resumed
```

Останні чотири події, зокрема події "Suspended" та "Resumed", є прямим наслідком перемикання поля `.spec.suspend`. Протягом цього часу ми бачимо, що жоден Pods не був створений, але створення Pod розпочалося знову, як тільки Job було відновлено.

### Змінні директиви планування {#mutable-scheduling-directives}

{{< feature-state for_k8s_version="v1.27" state="stable" >}}

У більшості випадків, паралелльні завдання вимагатимуть щоб їхні Podʼи запускались з певними обмеженнями, типу "всі в одній зоні" або "всі на GPU моделі x або y", але не комбінація обох.

[Поле призупинення](#suspending-a-job) є першим кроком для досягнення цих семантик. Поле призупинення дозволяє власному контролеру черги вирішити, коли повинна початися задача; Однак після того, як задача розпризупинена, власний контролер черги не впливає на те, де насправді буде розташований Pod задачі.

Ця функція дозволяє оновлювати директиви планування Job до запуску, що дає власному контролеру черги змогу впливати на розташування Podʼів, а в той же час здійснювати власне призначення Podʼів вузлам в kube-scheduler. Це дозволяється тільки для призупинених задач, які ніколи не були розпризупинені раніше.

Поля в шаблоні Podʼа задачі, які можна оновити, це приналежність до вузла, селектор вузла, толерантності, мітки, анотації та [вікно планування](/docs/concepts/scheduling-eviction/pod-scheduling-readiness/).

### Вказання власного селектора Podʼа {#specifying-your-own-pod-selector}

Зазвичай, коли ви створюєте обʼєкт задачі (Job), ви не вказуєте `.spec.selector`. Логіка системного визначення типових значень додає це поле під час створення задачі. Вона обирає значення селектора, яке не буде перекриватися з будь-якою іншою задачею.

Однак у деяких випадках вам може знадобитися перевизначити цей автоматично встановлений селектор. Для цього ви можете вказати `.spec.selector` задачі.

Будьте дуже уважні при цьому. Якщо ви вказуєте селектор міток, який не є унікальним для Podʼів цієї задачі, і який відповідає неповʼязаним Podʼам, то Podʼи з неповʼязаною задачею можуть бути видалені, або ця задача може вважати інші Podʼи завершеними, або одна чи обидві задачі можуть відмовитися від створення Podʼів або завершити роботу. Якщо вибираєте ненадійний селектор, то інші контролери (наприклад, ReplicationController) та їхні Podʼи можуть проявляти непередбачувану поведінку також. Kubernetes не зупинить вас від того, що ви можете зробити помилку при зазначені `.spec.selector`.

Ось приклад ситуації, коли вам може знадобитися використовувати цю функцію.

Скажімо, задача `old` вже запущена. Ви хочете, щоб існуючі Podʼи продовжували працювати, але ви хочете, щоб решта Podʼів, які вона створює, використовували інший шаблон Pod і щоб у задачі було нове імʼя. Ви не можете оновити задачу, оскільки ці поля не можна оновлювати. Отже, ви видаляєте задачу `old`, але *залишаєте її Podʼи запущеними*, використовуючи `kubectl delete jobs/old --cascade=orphan`. Перед видаленням ви робите помітку, який селектор вона використовує:

```shell
kubectl get job old -o yaml
```

Виввід схожий на цей:

```yaml
kind: Job
metadata:
  name: old
  ...
spec:
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Потім ви створюєте нову задачу з імʼям `new` і явно вказуєте той самий селектор. Оскільки існуючі Podʼи мають мітку `batch.kubernetes.io/controller-uid=a8f3d00d-c6d2-11e5-9f87-42010af00002`, вони також контролюються задачею `new`.

Вам потрібно вказати `manualSelector: true` в новій задачі, оскільки ви не використовуєте селектор, який система зазвичай генерує автоматично.

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  manualSelector: true
  selector:
    matchLabels:
      batch.kubernetes.io/controller-uid: a8f3d00d-c6d2-11e5-9f87-42010af00002
  ...
```

Сама нова задача матиме інший uid від `a8f3d00d-c6d2-11e5-9f87-42010af00002`. Встановлення `manualSelector: true` говорить системі, що ви розумієте, що робите, і дозволяє це неспівпадіння.

### Відстеження задачі за допомогою завершувачів {#job-tracking-with-finalizers}

{{< feature-state for_k8s_version="v1.26" state="stable" >}}

Панель управління стежить за Podʼами, які належать будь-якій задачі, і виявляє, чи
Pod був видалений з сервера API. Для цього контролер задачі створює Podʼи з завершувачем `batch.kubernetes.io/job-tracking`. Контролер видаляє завершувач тільки після того, як Pod був врахований в стані задачі, що дозволяє видалити Pod іншими контролерами або користувачами.

{{< note >}}
Див. [Мій Pod залишається в стані завершення](/docs/tasks/debug/debug-application/debug-pods/), якщо ви спостерігаєте, що Podʼи з задачі залишаються з завершувачем відстеження.
{{< /note >}}

### Еластичні індексовані задачі {#elastic-indexed-jobs}

{{< feature-state for_k8s_version="v1.27" state="beta" >}}

Ви можете масштабувати індексовані задачі вгору чи вниз, змінюючи як `.spec.parallelism`, так і `.spec.completions` разом, з умовою, що `.spec.parallelism == .spec.completions`. Коли увімкнено [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `ElasticIndexedJob` на [сервері API](/docs/reference/command-line-tools-reference/kube-apiserver/), `.spec.completions` є незмінним.

Сценарії використання для еластичних індексованих задач включають пакетні робочі навантаження, які вимагають масштабування індексованої задачі, такі як навчання MPI, Horovord, Ray, та PyTorch.

### Відкладене створення замінних Podʼів {#pod-replacement-policy}

{{< feature-state for_k8s_version="v1.29" state="beta" >}}

{{< note >}}
Ви можете встановити `podReplacementPolicy` для задач тільки у випадку, якщо увімкнутий [feature gate](/docs/reference/command-line-tools-reference/feature-gates/) `JobPodReplacementPolicy` (стандартно увімкнений).
{{< /note >}}

Типово контролер Job створює Podʼи якнайшвидше, якщо вони або зазнають невдачі, або знаходяться в стані завершення (мають відмітку видалення). Це означає, що в певний момент часу, коли деякі з Podʼів знаходяться в стані завершення, кількість робочих Podʼів для задачі може бути більшою, ніж `parallelism` або більше, ніж один Pod на індекс (якщо ви використовуєте індексовану задачу).

Ви можете вибрати створення замінних Podʼів лише тоді, коли Podʼи, які знаходиться в стані завершення, повністю завершені (мають `status.phase: Failed`). Для цього встановіть `.spec.podReplacementPolicy: Failed`. Типова політика заміщення залежить від того, чи задача має встановлену політику відмови Podʼів. Якщо для задачі не визначено політику відмови Podʼів, відсутність поля `podReplacementPolicy` вибирає політику заміщення `TerminatingOrFailed`: панель управління створює Podʼи заміни негайно після видалення Podʼів (як тільки панель управління бачить, що Pod для цієї задачі має встановлене значення `deletionTimestamp`). Для задач із встановленою політикою відмови Podʼів стандартне значення `podReplacementPolicy` — це `Failed`, інших значень не передбачено. Докладніше про політики відмови Podʼів для задач можна дізнатися у розділі [Політика відмови Podʼіви](#pod-failure-policy).

```yaml
kind: Job
metadata:
  name: new
  ...
spec:
  podReplacementPolicy: Failed
  ...
```

При увімкненому feature gate у вашому кластері ви можете перевірити поле `.status.terminating` задачі. Значення цього поля — це кількість Podʼів, якими володіє задача, які зараз перебувають у стані завершення.

```shell
kubectl get jobs/myjob -o yaml
```

```yaml
apiVersion: batch/v1
kind: Job
# .metadata and .spec omitted
status:
  terminating: 3 # три Podʼи, які перебувають у стані завершення і ще не досягли стану Failed
```

## Альтернативи {#alternatives}

### Тільки Podʼи {#bare-pods}

Коли вузол, на якому працює Pod, перезавантажується або виходить з ладу, Pod завершується і не буде перезапущений. Однак задача створить нові Podʼи для заміщення завершених. З цієї причини ми рекомендуємо використовувати задачу замість тільки Podʼів, навіть якщо ваш застосунок вимагає тільки рлного Podʼу.

### Контролер реплікації {#replication-controller}

Задачі є доповненням до [контролера реплікації](/docs/concepts/workloads/controllers/replicationcontroller/). Контролер реплікації керує Podʼами, які не повинні завершуватися (наприклад, вебсервери), і задача керує Podʼами, які очікують завершення (наприклад, пакетні завдання).

Якщо врахувати [життєвий цикл Podʼа](/docs/concepts/workloads/pods/pod-lifecycle/), `Job` *лише* придатний для Podʼів з `RestartPolicy`, рівним `OnFailure` або `Never`. (Примітка: Якщо `RestartPolicy` не встановлено, стандартне значення — `Always`.)

### Один Job запускає контролер Podʼів {#single-job-starts-controller-pod}

Ще один підхід — це те, що одна задача створює Podʼи, які своєю чергою створюють інші Podʼи, виступаючи як свого роду власний контролер для цих Podʼів. Це дає найбільшу гнучкість, але може бути дещо складним для початку використання та пропонує меншу інтеграцію з Kubernetes.

Один приклад такого підходу — це задача, яка створює Podʼи, яка виконує сценарій, який з свого боку запускає контролер Spark master (див. [приклад Spark](https://github.com/kubernetes/examples/tree/master/staging/spark/README.md)), запускає драйвер Spark, а потім робить очищення.

Перевагою цього підходу є те, що загальний процес отримує гарантію завершення обʼєкта задачі, але при цьому повністю контролюється те, які Podʼи створюються і яке навантаження їм призначається.

## {{% heading "whatsnext" %}}

- Дізнайтеся про [Podʼи](/docs/concepts/workloads/pods).
- Дізнайтеся про різні способи запуску задач:
  - [Груба паралельна обробка за допомогою робочої черги](/docs/tasks/job/coarse-parallel-processing-work-queue/)
  - [Дрібна паралельна обробка за допомогою робочої черги](/docs/tasks/job/fine-parallel-processing-work-queue/)
  - Використовуйте [індексовану задачу для паралельної обробки із статичним призначенням навантаження](/docs/tasks/job/indexed-parallel-processing-static/)
  - Створіть кілька задач на основі шаблону: [Паралельна обробка з використанням розширень](/docs/tasks/job/parallel-processing-expansion/)
- Перейдіть за посиланням [Автоматичне очищення завершених задач](#clean-up-finished-jobs-automatically), щоб дізнатися більше про те, як ваш кластер може очищати завершені та/або невдачливі завдання.
- `Job` є частиною REST API Kubernetes. Ознайомтесь з визначенням обʼєкта {{< api-reference page="workload-resources/job-v1" >}}, щоб зрозуміти API для задач.
- Прочитайте про [`CronJob`](/docs/concepts/workloads/controllers/cron-jobs/), який ви можете використовувати для визначення серії задач, які будуть виконуватися за розкладом, схожим на інструмент UNIX `cron`.
- Попрактикуйтесь в налаштувані обробки відмовних і невідмовних збоїв Podʼів за допомогою `podFailurePolicy` на основі покрокових [прикладів](/docs/tasks/job/pod-failure-policy/).
